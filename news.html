<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-130203500-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yifan Yang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="news.html" class="current">News</a></div>
<div class="menu-item"><a href="repositories.html">Repositories</a></div>
</td>
<td id="layout-content">
<h1>News</h1>
  <p>09/21/2025: Our SharpZO work regrading efficient VLM fine-tuning is accepted on NeurIPS 2025! <a href="https://neurips.cc/virtual/2025/poster/118465"> [Link]</a> </p> 
  <p>08/12/2025: My internship paper last summer about LLM Prunning is highlighed on Amazon Science! <a href="https://www.amazon.science/blog/a-better-path-to-pruning-large-language-models"> [Link]</a> </p> 
  <p>06/23/2025: I start my summer internship at AWS Agentic AI, working on improving the planning ability of multimodal AI agents. </p> 
  <p>06/11/2025: Finished my PhD proposal presentation. <a href="proposal.pdf"> [Slides]</a> </p>
  <p>05/15/2025: Two of our papers about LLMs pruning and parameter-efficient federated fine-tuning are accepted to ACL 2025 Findings. </p>
  <p>09/21/2024: Our paper 'AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning' is accepted by EMNLP 2024. </p>
  <p>04/21/2024: Our paper 'LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of
    Large Language Models' is selected as an oral presentation paper (top 5%) by NAACL 2024. </p>
  <p>03/28/2024: Our paper 'PID Control-Based Self-Healing to Improve the Robustness of Large Language Models' is accepted by TMLR. </p>
  <p>03/13/2024: Our paper 'LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of
    Large Language Models' is accepted by NAACL 2024. </p>
 <p>03/08/2024: I will join Amazon AGI for my summer internship, working on the inference speed-up of LLMs. </p>
   <p>08/01/2023: I start working in the field of Natural Language Processing, focusing on the efficient training of LLMs. </p>
</td>
</tr>
</table>
</body>
</html>
