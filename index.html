<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<script type="text/javascript">

var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
try {
    var pageTracker = _gat._getTracker("UA-130203500-1");
    pageTracker._trackPageview();
} catch(err) {}</script>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Yifan Yang</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="repositories.html">Repositories</a></div>
<!--<div class="menu-category">Research</div>-->
<!--<div class="menu-item"><a href="publications.html">Publications</a></div>-->
<!--<div class="menu-category">Teaching</div>-->
<!--<div class="menu-item"><a href="courses.html">Courses</a></div>-->
</td>
<td id="layout-content">
<h1>Yifan Yang</h1>
<table class="imgtable"><tr><td>
<img src="/profile.png" alt="Self" width="120px" />&nbsp;</td>
<td align="left"><p>PhD Candidate <br />
Department of Computer Science <br />
University of California, Santa Barbara (UCSB)<br />
Email: yifanyang at ucsb dot edu <br />
<a href="https://scholar.google.com/citations?user=PX2IQxsAAAAJ&amp;hl=en">Google Schoolar</a>&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.linkedin.com/in/yifan-yang-baa4a5138/">Linkedin</a></p>
</td></tr></table>
  <br />
<!-- <p><span style="color: red;"><b>I'm actively looking for a 2025 summer internship (both research and ML engineer) focused on LLM efficiency and general applications. I'm also interested in full-time positions starting in 2026. Feel free to reach out and keep in touch for future opportunities!</b></span></p> -->
<h2>About Me</h2>
<p>
  I am a PhD candidate in the UCSB Computer Science Department. Prior to UCSB, I received my B.S. in Electronic and Information Engineering from Huazhong University of Science and Technology (HUST) in China. 
  I'm working on both text and multimodal Large Language Models (LLMs), across a wide range of topics, including:
</p>
<ul>
  <li>Training and Inference Efficiency through model compression (Low-rank decomposition, pruning, quantization)</li>
  <li>Multimodal reasoning and AI agent</li>
  <li>Parameter-efficient fine-tuning (PEFT)</li>
  
</ul><h2>News</h2>
   <p>06/11/2025: I start my summer internship at AWS Agentic AI, working on improving the planning ability of multimodal AI agents. </p> 
  <p>06/11/2025: Finished my PhD proposal presentation. <a href="proposal.pdf"> [Slides]</a> </p>
  <p>05/15/2025: Two of our papers about LLMs pruning and parameter-efficient federated fine-tuning are accepted to ACL 2025 Findings. </p>
  <p>09/21/2024: Our paper 'AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning' is accepted by EMNLP 2024. </p>
  <p>04/21/2024: Our paper 'LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of
    Large Language Models' is selected as an oral presentation paper (top 5%) by NAACL 2024. </p>
  <p>03/28/2024: Our paper 'PID Control-Based Self-Healing to Improve the Robustness of Large Language Models' is accepted by TMLR. </p>
  <p>03/13/2024: Our paper 'LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of
    Large Language Models' is accepted by NAACL 2024. </p>
 <p>03/08/2024: I will join Amazon AGI for my summer internship, working on the inference speed-up of LLMs. </p>
   <p>08/01/2023: I start working in the field of Natural Language Processing, focusing on the efficient training of LLMs. </p>
<h2>Industrial Experience</h2>
    <p><b>AWS Agentic AI</b>, Applied Scientist Intern, Pasadena, CA, June 2025 - Sep 2025</p>
Working on enhancing the planning capabilities of multimodal AI agents. 
      <p><p>
  <p><b>Amazon AGI</b>, Applied Scientist Intern (Inclined), Pittsburgh, PA, June 2024 - Sep 2024</p>
  Working on low-degradation pruning method for inference speed up of large scale LLMs, refer to our Wanda++ paper on ACL 2025 for details.
<h2>Preprint</h2>
  Yifan Yang, Zhen Zhang, Rupak Vignesh Swaminathan, Jing Liu, Nathan Susanj, Zheng Zhang. "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes." <a href="hhttps://arxiv.org/abs/2506.20990"> [arxiv]</a>
    <p><p>
Jiayi Tian, Ryan Solgi, Jinming Lu, Yifan Yang, Hai Li, and Zheng Zhang. "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression." <a href="https://arxiv.org/pdf/2505.23966"> [arxiv]</a>
    <p><p>
Zhang, Zhen, Yifan Yang, Kai Zhen, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, and Zheng Zhang. "MaZO: Masked Zeroth-Order Optimization for Multi-Task Fine-Tuning of Large Language Models.", under review for ACL 25.  <a href="https://arxiv.org/pdf/2502.11513f"> [arxiv]</a>
    <p><p>
Zhou, Jiajun, Yifan Yang, Kai Zhen, Ziyue Liu, Yequan Zhao, Ershad Banijamali, Athanasios Mouchtaris, Ngai Wong, and Zheng Zhang. "QuZO: Quantized Zeroth-Order Fine-Tuning for Large Language Models.", under review for ACL 25.  <a href="https://www.arxiv.org/pdf/2502.12346"> [arxiv]</a>
  <p><p>
   Yifan Yang, Alec Koppel, Zheng Zhang, "A Gradient-based Approach for Online Robust Deep Neural Network Training with Noisy Labels".
   <a href="https://arxiv.org/abs/2306.05046.pdf"> [arxiv]</a>
<p><p>
   Yifan Yang, Chang Liu, Zheng Zhang, "Particle-based Online Bayesian Sampling", submitting to Transactions on Machine Learning Research (TMLR).
   <a href="https://arxiv.org/pdf/2302.14796.pdf"> [arxiv]</a>
<h2>Publications</h2>
  Yifan Yang, Kai Zhen, Bhavana Ganesh, Aram Galstyan, Goeric Huybrechts, Markus Müller, Jonas M. Kübler, Rupak Vignesh Swaminathan, Athanasios Mouchtaris, 
  Sravan Babu Bodapati, Nathan Susanj, Zheng Zhang, Jack FitzGerald, Abhishek Kumar, 
  "Wanda++: Pruning Large Language Models via Regional Gradients", in 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025 Findings), Austria and ICLR Workshop on Sparsity in LLMs, Singapore, 2025. 
  <a href="https://arxiv.org/abs/2503.04992"> [arxiv]</a>  <a href="https://yifan-yang.net/wandapp.github.io/"> [Project Page]</a> <a href="https://github.com/TTTTTTris/wandaplus"> [Unofffical Code]</a>
 <p><p> 
 Sajjad Ghiasvand, Yifan Yang, Zhiyu Xue, Mahnoosh Alizadeh, Zheng Zhang, Ramtin Pedarsani, "Communication-Efficient and Tensorized Federated Fine-Tuning of
Large Language Models", in 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025 Findings), Austria. <a href="https://arxiv.org/pdf/2410.13097.pdf"> [arxiv]</a>,
  <p><p>
  Yifan Yang, Kai Zhen, Ershad Banijamal, Athanasios Mouchtaris, Zheng Zhang, "AdaZeta: Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning", in Proceedings of
    2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024),  Miami, USA, 2024.
  <a href="https://arxiv.org/abs/2406.18060.pdf"> [arxiv]</a><a href="https://github.com/yifanycc/AdaZeta"> [code]</a>
<p><p>
Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang, "PID Control-Based Self-Healing to Improve the
    Robustness of Large Language Models", in Transactions on Machine Learning Research (TMLR), 2024.
<p><p>
   Yifan Yang, Jiajun Zhou, Ngai Wong, Zheng Zhang, "LoRETTA: Low-Rank Economic Tensor-Train Adaptation for
Ultra-Low-Parameter Fine-Tuning of Large Language Models", in Proceedings of
    2024 Annual Conference of the North American Chapter of the Association for Computational
    Linguistics (NAACL 2024), <b>Oral, top 5%</b>,  Mexico City, Mexico, 2024.<a href="https://arxiv.org/pdf/2402.11417.pdf"> [arxiv]</a><a href="https://github.com/yifanycc/loretta"> [code]</a>
<p><p>
      Yifan Yang, Lin Chen, Pan Zhou, Xiaofeng Ding, "Vflh: A Following-the-Leader-History Based Algorithm for Adaptive
    Online Convex Optimization with Stochastic Constraints", to appear in Proceedings of the 35th IEEE International
    Conference on Tools with Artificial Intelligence, Atlanta, USA, 2023. <b>(Best Student Paper Award, top 1%)</b>
   <p><p>
          Yifan Yang, Jie Xu, Zichuan Xu, Pan Zhou and Tie Qiu, "Quantile context-aware social IoT big data recommendation
    with D2D communication", IEEE Internet of Things Journal 7.6 (2020): 5533-5548.
    <div id="footer">
<div id="footer-text">
Page updated 09/20/2024.
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=200&t=n&d=AjYkGYhqfkljujHmVZFqGgw6WAoh_pTfdnSPzeqxwcE&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
